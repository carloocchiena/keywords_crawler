{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d585e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as BS4\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# global var\n",
    "MAX_THREADS = 50\n",
    "HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36'}\n",
    "\n",
    "# cleaner function to remove non-internal urls   \n",
    "def cleaner(data):\n",
    "    \"\"\"Cleans data from external url\"\"\"\n",
    "    for items in data:\n",
    "        if items[0:len(our_url)] == our_url:\n",
    "            temp_data.append(items)\n",
    "    global clean_data\n",
    "    clean_data = set(temp_data)\n",
    "\n",
    "# function to crawls the website and search for internal urls\n",
    "def crawler(url):\n",
    "    \"\"\"Scrape given url and search for internal urls\n",
    "\n",
    "    Args:\n",
    "        url (string): provide a complete url in the form \"https://www.example.com\"\n",
    "\n",
    "    Returns:\n",
    "        List: all the urls of the website\n",
    "    \"\"\"\n",
    "    global our_url\n",
    "    our_url = url\n",
    "\n",
    "    global temp_data\n",
    "    temp_data = []\n",
    "    \n",
    "    global clean_data\n",
    "    clean_data = []\n",
    "    \n",
    "    raw_data = []\n",
    "    temp = []  \n",
    "    limit = 1\n",
    "    count = 0\n",
    "\n",
    "    source_code = req.get(url, \"html.parser\", headers=HEADER).content\n",
    "\n",
    "    soup = BS4(source_code, 'lxml')\n",
    "\n",
    "    links = soup.findAll(\"a\", href=True)\n",
    "    \n",
    "    # append the internal urls to a list, iteratively\n",
    "    def raw_append(link):\n",
    "        raw_data.append(str(link.get(\"href\")))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        futures = [executor.submit(raw_append, link)for link in links] \n",
    "        \n",
    "    for i in raw_data:\n",
    "        try:\n",
    "            if i[0] == \"/\":\n",
    "                raw_data.append(url+i)\n",
    "        except Exception as e:\n",
    "            print(f\"[!!!] Exception occurred (a): {e}\")\n",
    "    \n",
    "    # call the cleaner function to purge the external urls        \n",
    "    cleaner(raw_data)\n",
    "    \n",
    "    # define a scraper function to run within the multithread executor\n",
    "    def scraper(link):\n",
    "        source_code = req.get(link, \"html.parser\").content\n",
    "        soup = BS4(source_code, 'lxml')\n",
    "        bites = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    while count < limit:    \n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "                futures = [executor.submit(scraper, link)for link in clean_data]\n",
    "                for result in as_completed(futures):\n",
    "                    temp.append(result.get(\"href\"))\n",
    "        except Exception as e:\n",
    "            print(f\"[!!!] Exception occurred (b): {e}\")      \n",
    "\n",
    "        cleaner(temp)\n",
    "        \n",
    "        # increase the counter, to stay within the limit and avoid very long running loops\n",
    "        count += 1\n",
    "         \n",
    "    return clean_data\n",
    "\n",
    "# lets' count the number of times a word appears in a given text within each url\n",
    "def kw_ranking (url):\n",
    "    \"\"\"Function to scan the text of each url and count the number of times a word appears\n",
    "\n",
    "    Args:\n",
    "        url (string): provide a complete url in the form \"https://www.example.com\"\n",
    "\n",
    "    Returns:\n",
    "        dict: link and kword count\n",
    "    \"\"\"    \n",
    "    # define our list of stopwords\n",
    "    custom = [\"!\",\"'\",\"£\",\"$\",\"%\",\"&\",\"(\",\")\",\"?\",\"^\",\"*\",\"+\",\"/\",\"-\", \"©\"]  # add further words to be excluded\n",
    "    stopwords_overall = stopwords.words('italian')  + stopwords.words('english') + custom\n",
    "    \n",
    "    # scrape the url\n",
    "    best = []\n",
    "           \n",
    "    page  = req.get(url, \"html.parser\", headers=HEADER).content\n",
    "\n",
    "    soup = BS4(page, \"lxml\")\n",
    "\n",
    "    text = soup.findAll(\"p\")\n",
    "\n",
    "    text_clean =[]\n",
    "    \n",
    "    # clean the text from unwanted characters\n",
    "    for slots in text:\n",
    "        text_clean.append(slots.get_text().lower())\n",
    "\n",
    "    text_all = \"\".join(text_clean).replace(\",\", \"\").replace(\".\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").split()\n",
    "\n",
    "    for word in reversed(text_all):\n",
    "        if word in stopwords_overall:\n",
    "            text_all.remove(word)\n",
    "\n",
    "    rank = Counter(text_all)\n",
    "\n",
    "    best.append(rank.most_common(5))\n",
    "    \n",
    "    named_kw = dict(zip(url, best))\n",
    "    \n",
    "    return named_kw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e78613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the given url and start the process \n",
    "url = \"https://www.google.com\"\n",
    "links = crawler(url)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61840e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our crawler\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = {executor.submit(kw_ranking, link): link for link in links}\n",
    "    for result in as_completed(futures):\n",
    "        link = futures.get(result)\n",
    "        try:\n",
    "            data=result.result()\n",
    "        except Exception as e:\n",
    "            print(f\"[!!!] Exception occurred (b): {e}\") \n",
    "        else:\n",
    "            print (f\"Link: {link}:\\nData:{data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
